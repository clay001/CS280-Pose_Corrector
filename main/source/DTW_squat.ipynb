{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute Dynamic Time Warp Distance of two sequences\n",
    "# http://alexminnaar.com/time-series-classification-and-clustering-with-python.html\n",
    "def DTWDistance(s1, s2):\n",
    "    DTW={}\n",
    "\n",
    "    for i in range(len(s1)):\n",
    "        DTW[(i, -1)] = float('inf')\n",
    "    for i in range(len(s2)):\n",
    "        DTW[(-1, i)] = float('inf')\n",
    "    DTW[(-1, -1)] = 0\n",
    "\n",
    "    for i in range(len(s1)):\n",
    "        for j in range(len(s2)):\n",
    "            dist= (s1[i]-s2[j])**2\n",
    "            DTW[(i, j)] = dist + min(DTW[(i-1, j)],DTW[(i, j-1)], DTW[(i-1, j-1)])\n",
    "\n",
    "    return np.sqrt(DTW[len(s1)-1, len(s2)-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now using the Dynamic time warping to compare the similarity to the standard action\n",
    "# the above action part(theta list) is stored in whole_actions\n",
    "#np.savetxt('a.txt',a,fmt='%0.8f')\n",
    "#for x in whole_actions:\n",
    "template = []\n",
    "template.append( np.loadtxt('squat_template/template_true1.txt',dtype=np.float32) )\n",
    "template.append( np.loadtxt('squat_template/template_true2.txt',dtype=np.float32) )\n",
    "template.append( np.loadtxt('squat_template/template_wrong0.txt',dtype=np.float32) )\n",
    "template.append( np.loadtxt('squat_template/template_wrong1.txt',dtype=np.float32) )   \n",
    "template_label = [1,1,0,0]\n",
    "\n",
    "test = []\n",
    "for i in range(15):\n",
    "    name  = 'squat_template/test'+str(i)+'.txt'\n",
    "    test.append( np.loadtxt(name,dtype=np.float32) )\n",
    "    \n",
    "truth = [1,1,1,0,0,1,1,1,1,1,1,0,0,0,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "46.91733058263925 good\n",
      "127.8400132531593 bad\n",
      "77.89947758540214 good\n",
      "129.55688623912275 bad\n",
      "251.7886017582089 good\n",
      "283.7508042984289 bad\n",
      "224.57508585707322 good\n",
      "143.2750429880242 bad\n",
      "24.31417922494161 good\n",
      "114.90219384135344 bad\n",
      "17.54120294583951 good\n",
      "139.02953733447532 bad\n",
      "19.82364996417209 good\n",
      "138.00215969761803 bad\n",
      "34.763288487331906 good\n",
      "134.58496706582025 bad\n",
      "60.11296676434253 good\n",
      "156.71962372420646 bad\n",
      "6.363961030678928 good\n",
      "158.5538731364354 bad\n",
      "6.363961030678928 good\n",
      "168.66057328285518 bad\n",
      "252.74742863944766 good\n",
      "144.2653804625351 bad\n",
      "224.81318615104203 good\n",
      "140.15620752006058 bad\n",
      "29.213668220616473 good\n",
      "136.1184168525202 bad\n",
      "21.353951107498258 good\n",
      "153.6413142399216 bad\n",
      "[1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         Bad       1.00      0.50      0.67         6\n",
      "        Good       0.75      1.00      0.86         9\n",
      "\n",
      "   micro avg       0.80      0.80      0.80        15\n",
      "   macro avg       0.88      0.75      0.76        15\n",
      "weighted avg       0.85      0.80      0.78        15\n",
      "\n"
     ]
    }
   ],
   "source": [
    "predictions = []\n",
    "for j in range(len(test)):\n",
    "    f1_good = [] \n",
    "    f1_bad  = []\n",
    "    # Compare distance of current test example with all training examples\n",
    "    for i in range(len(template)):\n",
    "        dist1 = DTWDistance(template[i], test[j])\n",
    "        if template_label[i]:\n",
    "            f1_good.append(dist1)\n",
    "        else:\n",
    "            f1_bad.append(dist1)\n",
    "    good_score = np.mean(f1_good) \n",
    "    bad_score = np.mean(f1_bad) \n",
    "    print(good_score , \"good\")\n",
    "    print(bad_score , \"bad\")\n",
    "    if good_score <= bad_score:\n",
    "        predictions.append(1)\n",
    "    else:\n",
    "        predictions.append(0)\n",
    "print(predictions)\n",
    "#print(classification_report([0,1,1], predictions, target_names=['correct', 'incorrect']))\n",
    "target_names = ['Bad', 'Good']\n",
    "print(classification_report(truth, predictions, target_names=target_names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "truth = [1,1,1,0,0,1,1,1,1,1,1,0,0,0]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
